%%
%% pytorch-neural-doodle/docs/content/chapters/introduction.tex
%%
%% Created by Paul Warkentin <paul@warkentin.email> on 21/08/2018.
%% Updated by Bastian Boll <mail@bbboll.com> on 02/10/2018.
%%

\section{Introduction}
\label{section:introduction}

% TODO: what exactly is style transfer anyway?

In light of meaningful advances in the field of image classification using convolutional neural networks (CNNs) \cite{vgg2014}, interest has been sparked to exploit the inverse process for image generation. Because the intrinsic dimension of the input image space \(I\) is generally much larger than the intrinsic dimension of the label space \(L\), such a CNN cannot canonically be a bijective mapping. The inverse process can therefore be seen as selecting an element \(x\) of the preimage \(\mathrm{CNN}^{-1}(y)\) for a fixed label \(y\in L\) such that a measure of visual or artistic appeal is maximized.

For the purpose of a more fine-grained analysis of this process, suppose a given network which is discriminatively trained for object classification. Let \(L_i\) denote the feature space of said network at layer \(i\). The network can be characterized as functions \(\text{net}_i\colon I \to L_i\) which encode a given input image into the feature space \(L_i\). Sampling the preimage \(\text{net}_i^{-1}(y_i)\) of a given feature vector \(y_i\in L_i\) can serve to visuallize the way in which such networks extract features from the input image. One can observe that the lower levels of the network capture mostly local texture information while higher levels encode more abstract information about the objects visible in the input image.

% Leveraging encoding into these feature spaces for style transfer requires the combination of two general fields of expertise: style / texture and content.

In texture generation, a related area of research, efforts have been made to characterize texture as image statistics \cite{julesz1962visual,heeger1995pyramid,rosenholtz2012summary}. Given a textured image, one can generate a different image which human observers percept to contain the same texture. This is done by looking for an image that reproduces the respective image statistics. However, careful hand-crafting of such image statistics is inherently limited and the results still fall short of being able to reproduce the full range of natural texture. In 2015, Gatys et al \cite{gatys2015texture} proposed using the feature encoding in lower levels of a VGG19 network \cite{vgg2014} to provide the image statistics needed to capture texture. Training the network to classify objects also results in functions \(\text{net}_i\) which encode low level image statistics such as texture. 

In the same year, Gatys et al also introduced a precise formulation of artistic style transfer as reproducing a given content image using texture from a different style image \cite{gatys2015neural}. They achieve very convincing results by constructing an image which reproduces the low level texture statistics of the style image while also reproducing higher level statistics of the content image.
%More specifically 

% classification breakthrough progress
% generation of images using CNNs
% specifically: reproduce statistics of given template image, e.g. related to texture
% augmented task: combine 
% 	(global semantic information / high level statistics / content) from one image with 
% 	(local texture information / low level statistics / style) from another image.
% Statistics are computed from activations of layers in a pre-trained classification network. Gram matrices, empirical correlation
% glitches due to lack of semantic segmentation 
% incorporate user provided (segmantation / semantic map / doodle) into style transfer
% this also gives user artistic control
% interactive nature of the algorithm renders skillful creation of neural doodles necessary

% Progression of this field seems to go like:
% 1. CNN for classification
% 2. CNN for generating images (especially the VGG architecture 2014)
% 3. Gatys et al introduce formulation of style transfer and respective content loss function (Gram matrices, global empirical correlation)
% 4. Patch based approaches are introduced as means of more precisely capturing local pixel statistics
% 5. Champandard builds on the patch based approach to incorporate user input in the form of semantic segmentation maps (neural doodles) 
% 6. Ulyanov et al (2016) introduce an alternative approach based on texture synthesis which mitigates computational cost and memory requirements necessary in the architectures above. This is claimed to yield similar results 500x faster and using an order of magnitude less memory. (see this blog post: https://dmitryulyanov.github.io/feed-forward-neural-doodle/). This is done by exploring the feasible set of images using a generative neural network instead of a classical optimization algorithm (Gatys uses L-BFGS). He also gives a much more convincing statistical motivation for the loss construction and his approach seems to otherwise be mostly orthogonal to the concepts introduced above.